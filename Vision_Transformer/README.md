# Vision Transformer (ViT) Implementation

A PyTorch implementation of the Vision Transformer from the paper ["An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale"](https://arxiv.org/abs/2010.11929) by Dosovitskiy et al.

## ğŸ“‹ Overview

This project is part of my **papers-to-code** repository series, where I replicate influential research papers in PyTorch. The Vision Transformer revolutionized computer vision by applying the transformer architecture directly to image patches, achieving state-of-the-art results on image classification tasks.

## ğŸ—ï¸ Architecture

The implementation follows the original ViT architecture:

1. **Patch Embedding**: Images are split into fixed-size patches and linearly embedded
2. **Positional Encoding**: Learnable position embeddings are added to patch embeddings
3. **Transformer Encoder**: Standard transformer encoder layers with multi-head self-attention
4. **Classification Head**: Final linear layer for classification

```
Input Image (224x224x3) 
    â†“
Patch Embedding (16x16 patches) 
    â†“
Position Embedding + [CLS] Token
    â†“
Transformer Encoder (Ã—12 layers)
    â†“
Classification Head
    â†“
Output Classes
```

## ğŸ“ Project Structure

```
Vision_Transformer/
â”œâ”€â”€ train.py                    # Main training script
â”œâ”€â”€ VIT_Paper_Replicating.ipynb # Research notebook with analysis
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ dataset.py              # CIFAR-10 data loading and preprocessing
â”‚   â”œâ”€â”€ plot_curves.py          # Training curve visualization
â”‚   â””â”€â”€ train_test_fn.py        # Training and testing functions
â””â”€â”€ vit_model/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ vit.py                  # Main ViT model
    â”œâ”€â”€ embedding.py            # Patch embedding layer
    â”œâ”€â”€ encoder.py              # Transformer encoder block
    â”œâ”€â”€ mlp.py                  # Multi-layer perceptron
    â””â”€â”€ msa.py                  # Multi-head self-attention
```

## ğŸš€ Quick Start

### Prerequisites

```bash
pip install torch torchvision matplotlib tqdm
```

### Training

```bash
python train.py
```

The script trains on a subset of CIFAR-10 (2000 training samples, 300 test samples) for demonstration purposes.

## ğŸ”§ Model Configuration

Current configuration (ViT-Base/16):

| Parameter | Value |
|-----------|-------|
| Image Size | 224Ã—224 |
| Patch Size | 16Ã—16 |
| Embedding Dimension | 768 |
| Transformer Layers | 12 |
| Attention Heads | 12 |
| MLP Size | 3072 |
| Dropout | 0.1 |

## ğŸ“Š Training Details

- **Dataset**: CIFAR-10 (subset for quick training)
- **Optimizer**: Adam (lr=1e-3, weight_decay=0.1)
- **Loss Function**: CrossEntropyLoss
- **Epochs**: 10
- **Batch Size**: 32

## ğŸ” Key Components

### Patch Embedding (`embedding.py`)
- Converts 2D images into 1D sequences of patch embeddings
- Adds learnable [CLS] token and positional embeddings
- Includes dropout for regularization

### Multi-Head Self-Attention (`msa.py`)
- Implements the attention mechanism from the original transformer
- Uses PyTorch's native MultiheadAttention module
- Includes layer normalization

### MLP Block (`mlp.py`)
- Feed-forward network with GELU activation
- Dropout for regularization
- Layer normalization as per the original paper

### Transformer Encoder (`encoder.py`)
- Combines MSA and MLP blocks
- Implements residual connections
- Stacks multiple layers for deep processing

## ğŸ“ˆ Results

The model trains on CIFAR-10 and tracks:
- Training/Test Loss
- Training/Test Accuracy
- Training time

Results are visualized using the plotting utilities in `utils/plot_curves.py`.

## ğŸ”¬ Research Notes

Detailed analysis and experiments can be found in `VIT_Paper_Replicating.ipynb`, including:
- Paper walkthrough and key insights
- Architecture analysis
- Experimental results and comparisons
- Implementation details and design decisions

## ğŸ¯ Future Improvements

- [ ] Add support for different ViT variants (ViT-Large, ViT-Huge)
- [ ] Implement pre-training on larger datasets
- [ ] Add data augmentation strategies
- [ ] Support for different image sizes
- [ ] Model checkpointing and resuming
- [ ] Evaluation on ImageNet

## ğŸ“š References

```bibtex
@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}
```

## ğŸ¤ Contributing

This is part of my research paper replication series. Feel free to:
- Open issues for bugs or improvements
- Submit pull requests for enhancements
- Suggest other papers to implement

## ğŸ“„ License

MIT License - see LICENSE file for details.

---

**Part of the papers-to-code series** ğŸ“ | Replicating influential AI research papers in PyTorch
